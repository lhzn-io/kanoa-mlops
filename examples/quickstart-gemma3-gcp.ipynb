{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d71bb2",
   "metadata": {},
   "source": [
    "## Step 1: Deploy Gemma 3\n",
    "\n",
    "Choose your model size based on your needs:\n",
    "\n",
    "```bash\n",
    "# Fast iteration, good quality\n",
    "make deploy-gemma3-4b\n",
    "\n",
    "# Better quality, still reasonable speed\n",
    "make deploy-gemma3-12b\n",
    "\n",
    "# Best quality, slower and more expensive\n",
    "make deploy-gemma3-27b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597f01db",
   "metadata": {},
   "source": [
    "## 1. Configure Your Project\n",
    "\n",
    "Set your GCP project ID and Hugging Face token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure your GCP project\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # <-- CHANGE THIS\n",
    "\n",
    "# Hugging Face token (required for Gemma - gated model)\n",
    "# Get yours at: https://huggingface.co/settings/tokens\n",
    "# Then accept Gemma terms at: https://huggingface.co/google/gemma-3-4b-it\n",
    "HF_TOKEN = \"hf_...\"  # <-- CHANGE THIS\n",
    "\n",
    "# Optional: Override region (default: us-central1)\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Choose model size: gemma3-4b, gemma3-12b, or gemma3-27b\n",
    "MODEL_PRESET = \"gemma3-4b\"\n",
    "\n",
    "os.environ[\"TF_VAR_project_id\"] = PROJECT_ID\n",
    "os.environ[\"TF_VAR_region\"] = REGION\n",
    "os.environ[\"TF_VAR_hf_token\"] = HF_TOKEN\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region:  {REGION}\")\n",
    "print(f\"Model:   {MODEL_PRESET}\")\n",
    "print(f\"HF Token: {'Set' if HF_TOKEN.startswith('hf_') else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec34ac5",
   "metadata": {},
   "source": [
    "## 2. Deploy Infrastructure\n",
    "\n",
    "Initialize Terraform and deploy the Gemma preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e765595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../infrastructure/gcp\n",
    "\n",
    "# Initialize Terraform (only needed once)\n",
    "terraform init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL_PRESET\"\n",
    "cd ../infrastructure/gcp\n",
    "\n",
    "# Deploy using selected Gemma preset\n",
    "terraform apply -var-file=presets/$1.tfvars -auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff85ff8",
   "metadata": {},
   "source": [
    "## 3. Get API Endpoint\n",
    "\n",
    "Retrieve the vLLM server URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f88160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "# Get Terraform outputs\n",
    "result = subprocess.run(\n",
    "    [\"terraform\", \"output\", \"-json\"],\n",
    "    check=False,\n",
    "    cwd=\"../infrastructure/gcp\",\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "outputs = json.loads(result.stdout)\n",
    "\n",
    "API_ENDPOINT = outputs[\"api_endpoint\"][\"value\"]\n",
    "print(f\"API Endpoint: {API_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e058d",
   "metadata": {},
   "source": [
    "## 4. Wait for Server Ready\n",
    "\n",
    "The vLLM server takes ~3-5 minutes to download and load the model. Run this cell to wait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def wait_for_server(endpoint: str, timeout: int = 600) -> bool:\n",
    "    \"\"\"Wait for vLLM server to become ready.\"\"\"\n",
    "    health_url = f\"{endpoint}/health\"\n",
    "    start = time.time()\n",
    "\n",
    "    print(f\"Waiting for server at {health_url}...\")\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            resp = requests.get(health_url, timeout=5)\n",
    "            if resp.status_code == 200:\n",
    "                print(f\"\\nServer ready! ({int(time.time() - start)}s)\")\n",
    "                return True\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(f\"\\nTimeout after {timeout}s\")\n",
    "    return False\n",
    "\n",
    "\n",
    "wait_for_server(API_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42684bf5",
   "metadata": {},
   "source": [
    "## 5. Run Inference with kanoa\n",
    "\n",
    "Now let's use the kanoa library to interact with Gemma 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ca2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kanoa.backends.vllm import VLLMBackend\n",
    "\n",
    "# Model names by preset\n",
    "MODEL_NAMES = {\n",
    "    \"gemma3-4b\": \"google/gemma-3-4b-it\",\n",
    "    \"gemma3-12b\": \"google/gemma-3-12b-it\",\n",
    "    \"gemma3-27b\": \"google/gemma-3-27b-it\",\n",
    "}\n",
    "\n",
    "# Initialize the backend\n",
    "backend = VLLMBackend(api_base=API_ENDPOINT, model=MODEL_NAMES[MODEL_PRESET])\n",
    "\n",
    "print(f\"Connected to: {backend.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e62b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text query\n",
    "response = backend.generate(\n",
    "    prompt=\"What is machine learning? Explain in 2 sentences.\", max_tokens=100\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923920cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation\n",
    "response = backend.generate(\n",
    "    prompt=\"Write a Python function to calculate the Fibonacci sequence up to n terms.\",\n",
    "    max_tokens=300,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning task\n",
    "response = backend.generate(\n",
    "    prompt=\"\"\"Solve this step by step:\n",
    "A train travels from City A to City B at 60 mph.\n",
    "The return trip is at 40 mph.\n",
    "What is the average speed for the entire round trip?\"\"\",\n",
    "    max_tokens=300,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0087f4f",
   "metadata": {},
   "source": [
    "## 6. Cost Tracking\n",
    "\n",
    "| Model | GPU | Cost/Hour |\n",
    "|-------|-----|----------|\n",
    "| gemma3-4b | L4 (24GB) | ~$0.70 |\n",
    "| gemma3-12b | L4 (24GB) | ~$0.70 |\n",
    "| gemma3-27b | A100 (80GB) | ~$3.00 |\n",
    "\n",
    "The server has a 30-minute idle timeout - if no requests are made, it will shut down automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check instance status\n",
    "!gcloud compute instances describe vllm-server --zone={REGION}-a --format=\"value(status)\" 2>/dev/null || echo \"Instance not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa295113",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "\n",
    "**Important**: Destroy the infrastructure when done to avoid charges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$MODEL_PRESET\"\n",
    "cd ../infrastructure/gcp\n",
    "\n",
    "# Destroy all resources\n",
    "terraform destroy -var-file=presets/$1.tfvars -auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c81b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Molmo 7B** for multimodal tasks: [quickstart-molmo.ipynb](./quickstart-molmo.ipynb)\n",
    "- See [infrastructure/gcp/README.md](../infrastructure/gcp/README.md) for more configuration options\n",
    "- Check [docs/User_Setup_Guide.md](../docs/User_Setup_Guide.md) for detailed setup instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c616f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check deployment status\n",
    "!cd /home/lhzn/Projects/lhzn-io/kanoa-mlops && make status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66197c7f",
   "metadata": {},
   "source": [
    "## Step 2: Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THESE after deployment!\n",
    "VLLM_API_BASE = \"http://YOUR_EXTERNAL_IP:8000/v1\"\n",
    "\n",
    "# Choose the model you deployed:\n",
    "MODEL_NAME = \"google/gemma-3-4b-it\"  # For deploy-gemma3-4b\n",
    "# MODEL_NAME = \"google/gemma-3-12b-it\"   # For deploy-gemma3-12b\n",
    "# MODEL_NAME = \"google/gemma-3-27b-it\"   # For deploy-gemma3-27b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "\n",
    "health_url = VLLM_API_BASE.replace(\"/v1\", \"/health\")\n",
    "try:\n",
    "    response = requests.get(health_url, timeout=5)\n",
    "    print(\n",
    "        \"✓ vLLM server is healthy!\"\n",
    "        if response.ok\n",
    "        else f\"⚠ Status: {response.status_code}\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"✗ Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f1270f",
   "metadata": {},
   "source": [
    "## Step 3: Create Sample Data & Visualization\n",
    "\n",
    "Let's create a more complex business scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate realistic business metrics\n",
    "np.random.seed(123)\n",
    "dates = pd.date_range(\"2024-01-01\", periods=90, freq=\"D\")\n",
    "\n",
    "# Simulate user engagement metrics\n",
    "base_users = 10000\n",
    "daily_active_users = (\n",
    "    base_users\n",
    "    + np.cumsum(np.random.randn(90) * 100)\n",
    "    + np.sin(np.arange(90) * 2 * np.pi / 7) * 500\n",
    ")\n",
    "session_duration = 5 + np.random.randn(90) * 0.5 + np.linspace(0, 1, 90)  # Trending up\n",
    "conversion_rate = 0.03 + np.random.randn(90) * 0.005\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": dates,\n",
    "        \"daily_active_users\": daily_active_users.astype(int),\n",
    "        \"avg_session_minutes\": session_duration,\n",
    "        \"conversion_rate\": conversion_rate.clip(0, 0.1),\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fab872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dashboard\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Panel 1: DAU with 7-day rolling average\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(df[\"date\"], df[\"daily_active_users\"], alpha=0.4, label=\"Daily\")\n",
    "ax1.plot(\n",
    "    df[\"date\"],\n",
    "    df[\"daily_active_users\"].rolling(7).mean(),\n",
    "    linewidth=2,\n",
    "    label=\"7-day avg\",\n",
    ")\n",
    "ax1.set_title(\"Daily Active Users\")\n",
    "ax1.set_ylabel(\"Users\")\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Panel 2: Session duration trend\n",
    "ax2 = axes[0, 1]\n",
    "ax2.fill_between(df[\"date\"], df[\"avg_session_minutes\"], alpha=0.3)\n",
    "ax2.plot(\n",
    "    df[\"date\"], df[\"avg_session_minutes\"].rolling(7).mean(), linewidth=2, color=\"green\"\n",
    ")\n",
    "ax2.set_title(\"Avg Session Duration (minutes)\")\n",
    "ax2.set_ylabel(\"Minutes\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Panel 3: Conversion rate\n",
    "ax3 = axes[1, 0]\n",
    "colors = [\n",
    "    \"green\" if x > df[\"conversion_rate\"].mean() else \"red\"\n",
    "    for x in df[\"conversion_rate\"]\n",
    "]\n",
    "ax3.bar(df[\"date\"], df[\"conversion_rate\"] * 100, color=colors, alpha=0.6, width=1)\n",
    "ax3.axhline(\n",
    "    df[\"conversion_rate\"].mean() * 100,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Avg: {df['conversion_rate'].mean() * 100:.2f}%\",\n",
    ")\n",
    "ax3.set_title(\"Conversion Rate (%)\")\n",
    "ax3.set_ylabel(\"Percent\")\n",
    "ax3.legend()\n",
    "ax3.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Panel 4: Correlation heatmap-style scatter\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(\n",
    "    df[\"daily_active_users\"],\n",
    "    df[\"conversion_rate\"] * 100,\n",
    "    c=df[\"avg_session_minutes\"],\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.colorbar(scatter, ax=ax4, label=\"Session Duration (min)\")\n",
    "ax4.set_xlabel(\"Daily Active Users\")\n",
    "ax4.set_ylabel(\"Conversion Rate (%)\")\n",
    "ax4.set_title(\"DAU vs Conversion (colored by session length)\")\n",
    "\n",
    "plt.suptitle(\"Product Engagement Dashboard - Q1 2024\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e8cb3",
   "metadata": {},
   "source": [
    "## Step 4: Analyze with Gemma 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kanoa.backends import VLLMBackend\n",
    "\n",
    "backend = VLLMBackend(\n",
    "    api_base=VLLM_API_BASE,\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"✓ Connected to {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dashboard analysis\n",
    "result = backend.interpret(\n",
    "    fig=fig,\n",
    "    data=df.describe(),  # Pass summary statistics\n",
    "    context=\"Product engagement dashboard for a SaaS application. Q1 2024 data.\",\n",
    "    focus=\"Provide executive summary: key trends, concerns, and recommendations.\",\n",
    "    kb_context=None,\n",
    "    custom_prompt=None,\n",
    ")\n",
    "\n",
    "print(\"Executive Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18a9b0",
   "metadata": {},
   "source": [
    "## Step 5: Specialized Analysis Prompts\n",
    "\n",
    "Gemma 3 excels at following specific instructions. Let's try different analysis angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis focus\n",
    "result = backend.interpret(\n",
    "    fig=fig,\n",
    "    data=df.describe(),\n",
    "    context=\"Product metrics dashboard.\",\n",
    "    focus=\"Identify statistical patterns: seasonality, trends, correlations, outliers.\",\n",
    "    kb_context=None,\n",
    "    custom_prompt=None,\n",
    ")\n",
    "\n",
    "print(\"Statistical Analysis\")\n",
    "print(\"-\" * 40)\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompt for specific format\n",
    "custom_prompt = \"\"\"\n",
    "Analyze this dashboard and provide your response in this exact format:\n",
    "\n",
    "## Key Metrics Summary\n",
    "- DAU: [trend direction] from [start] to [end]\n",
    "- Session Duration: [trend direction]\n",
    "- Conversion: [assessment]\n",
    "\n",
    "## Top 3 Insights\n",
    "1. [Most important finding]\n",
    "2. [Second finding]\n",
    "3. [Third finding]\n",
    "\n",
    "## Recommended Actions\n",
    "1. [Immediate action]\n",
    "2. [Short-term action]\n",
    "3. [Long-term strategy]\n",
    "\"\"\"\n",
    "\n",
    "result = backend.interpret(\n",
    "    fig=fig,\n",
    "    data=None,\n",
    "    context=\"SaaS product engagement metrics.\",\n",
    "    focus=None,\n",
    "    kb_context=None,\n",
    "    custom_prompt=custom_prompt,\n",
    ")\n",
    "\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92558da1",
   "metadata": {},
   "source": [
    "## Step 6: Compare with Different Model Sizes\n",
    "\n",
    "If you want to compare outputs from different model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506652ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare models (deploy each first)\n",
    "def compare_models(fig, models_and_endpoints):\n",
    "    \"\"\"Compare interpretations from different models.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, (api_base, model_name) in models_and_endpoints.items():\n",
    "        try:\n",
    "            backend = VLLMBackend(api_base=api_base, model=model_name)\n",
    "            result = backend.interpret(\n",
    "                fig=fig,\n",
    "                data=None,\n",
    "                context=\"Product metrics dashboard.\",\n",
    "                focus=\"Summarize in 2-3 sentences.\",\n",
    "                kb_context=None,\n",
    "                custom_prompt=None,\n",
    "            )\n",
    "            results[name] = result.text\n",
    "        except Exception as e:\n",
    "            results[name] = f\"Error: {e}\"\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (uncomment and modify endpoints):\n",
    "# models = {\n",
    "#     \"Gemma 3 4B\": (\"http://IP1:8000/v1\", \"google/gemma-3-4b-it\"),\n",
    "#     \"Gemma 3 12B\": (\"http://IP2:8000/v1\", \"google/gemma-3-12b-it\"),\n",
    "# }\n",
    "# results = compare_models(fig, models)\n",
    "# for name, text in results.items():\n",
    "#     print(f\"\\n{name}:\")\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f2739",
   "metadata": {},
   "source": [
    "## Step 7: Working with Real Data\n",
    "\n",
    "Example of loading and visualizing your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for loading your own data\n",
    "def analyze_csv(filepath, x_col, y_cols, title=\"My Data\"):\n",
    "    \"\"\"Load CSV and create visualization for analysis.\"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for col in y_cols:\n",
    "        ax.plot(df[x_col], df[col], label=col, marker=\"o\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Get interpretation\n",
    "    result = backend.interpret(\n",
    "        fig=fig,\n",
    "        data=df.describe(),\n",
    "        context=f\"Analysis of {title}\",\n",
    "        focus=\"Describe the key patterns and provide insights.\",\n",
    "        kb_context=None,\n",
    "        custom_prompt=None,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    return result.text\n",
    "\n",
    "\n",
    "# Example:\n",
    "# analysis = analyze_csv(\"my_data.csv\", x_col=\"date\", y_cols=[\"revenue\", \"costs\"], title=\"Financial Data\")\n",
    "# print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81457e3b",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "```bash\n",
    "# Stop (keeps disk for quick restart)\n",
    "make stop\n",
    "\n",
    "# Or fully destroy (no more charges)\n",
    "make destroy\n",
    "\n",
    "# Destroy ALL deployments\n",
    "make clean-infra\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fa41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to destroy\n",
    "# !cd /home/lhzn/Projects/lhzn-io/kanoa-mlops && make destroy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469f07a",
   "metadata": {},
   "source": [
    "## Tips for Best Results with Gemma 3\n",
    "\n",
    "1. **Be specific in `focus`**: \"Identify outliers\" works better than \"analyze this\"\n",
    "2. **Use `custom_prompt` for formatting**: Gemma 3 follows structured prompts well\n",
    "3. **Include `data` parameter**: Summary stats help ground the analysis\n",
    "4. **Temperature**: Lower (0.3) for factual, higher (0.8) for creative interpretations\n",
    "\n",
    "## Model Selection Guide\n",
    "\n",
    "| Use Case | Recommended |\n",
    "|----------|-------------|\n",
    "| Quick iteration, prototyping | Gemma 3 4B |\n",
    "| Production analysis | Gemma 3 12B |\n",
    "| Complex reasoning, reports | Gemma 3 27B |\n",
    "| Chart understanding | Molmo-7B |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
