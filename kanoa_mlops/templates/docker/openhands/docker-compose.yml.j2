services:
  openhands:
    image: ${OPENHANDS_IMAGE:-docker.openhands.dev/openhands/openhands:1.1.0}
    container_name: kanoa-openhands
    environment:
      - SANDBOX_RUNTIME_CONTAINER_IMAGE=${SANDBOX_RUNTIME_CONTAINER_IMAGE:-docker.openhands.dev/openhands/runtime:1.1.0-nikolaik}
      - WORKSPACE_MOUNT_PATH=${WORKSPACE_BASE:-$PWD/workspace}
      - LLM_API_KEY=${LLM_API_KEY:-ollama}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://host.docker.internal:11434}
      - LLM_MODEL=${LLM_MODEL:-ollama/devstral-small-2:24b} # Swapped to your coding model
      - LLM_NUM_CTX=32768 # CRITICAL: Increases the memory/context window
      - LLM_TIMEOUT=300   # Devstral 24B may take a moment to "think" on a Tegra/Mac
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
{% if arch_config.platform_name.startswith("macos") %}
      - "host.orbstack.internal:host-gateway"
{% endif %}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ~/.openhands:/.openhands
      - ${WORKSPACE_BASE:-$PWD/workspace}:/opt/workspace_base
    stdin_open: true
    tty: true

networks:
  default:
    name: kanoa-mlops
