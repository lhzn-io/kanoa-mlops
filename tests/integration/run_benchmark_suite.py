#!/usr/bin/env python3
"""Run multiple benchmark iterations and compute statistics.

Unified benchmark suite runner that works with any model test.
"""

import argparse
import contextlib  # Ensure contextlib is imported at the top-level
import json
import os
import subprocess
import sys
import tempfile
import time
from pathlib import Path
from statistics import mean, stdev


def run_single_benchmark(test_script: str, results_file: str):
    """Run a single benchmark and return results."""
    # Ensure absolute path for the results file so the subprocess finds it
    results_path = (Path(__file__).parent / results_file).resolve()

    # Pass the output location to the subprocess via environment variable
    env = os.environ.copy()
    env["RESULTS_FILE"] = str(results_path)

    result = subprocess.run(
        ["python3", test_script],
        check=False,
        capture_output=True,
        text=True,
        cwd=Path(__file__).parent,
        env=env,
    )

    if result.returncode != 0:
        print(f"[ERROR] Benchmark failed: {result.stderr}")
        return None

    # Load the results
    try:
        with open(results_path, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"[ERROR] {results_file} not found")
        # If the script failed silently or didn't write the file
        if result.stdout:
            print(f"Stdout: {result.stdout[:200]}...")
        return None


def main():
    parser = argparse.ArgumentParser(
        description="Run benchmark suite for a model",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python3 run_benchmark_suite.py --model gemma3 --runs 5
  python3 run_benchmark_suite.py --model molmo --runs 10
  python3 run_benchmark_suite.py --model ollama --runs 3
  python3 run_benchmark_suite.py --model olmo3 --runs 5
        """,
    )
    parser.add_argument(
        "--model",
        required=True,
        choices=[
            "gemma3",
            "molmo",
            "ollama",
            "olmo3",
            "text-comparison",
            "vision-comparison",
        ],
        help="Model to benchmark",
    )
    parser.add_argument(
        "--runs", type=int, default=3, help="Number of benchmark runs (default: 3)"
    )
    parser.add_argument(
        "--warmup",
        type=int,
        default=0,
        help="Number of warmup runs to discard (default: 0)",
    )
    parser.add_argument(
        "--test-script",
        help="Override test script name (default: test_vllm_{model}_api.py or test_ollama_{model}.py)",
    )
    parser.add_argument(
        "--results-file",
        help="Override results filename (default: benchmark_results.json or benchmark_results_ollama.json)",
    )
    parser.add_argument(
        "--output",
        help="Override output filename (default: benchmark_statistics_{model}.json)",
    )

    args = parser.parse_args()

    # Set defaults based on model
    if args.test_script is None:
        if args.model == "ollama":
            # Default to the text comparison script for generic ollama runs if not specified
            args.test_script = "test_text_comparison.py"
        elif args.model in ["text-comparison", "vision-comparison"]:
            # Remove the 'vllm_' prefix from the constructed filename
            script_base = args.model.replace("-", "_")
            args.test_script = f"test_{script_base}.py"
        else:
            args.test_script = f"test_vllm_{args.model}_specific.py"

    # Use a temporary file for intermediate results if not specified
    is_temp_file = False
    if args.results_file is None:
        is_temp_file = True
        fd, temp_path = tempfile.mkstemp(suffix=".json", prefix="bench_run_")
        os.close(fd)
        args.results_file = temp_path
        # We'll use the absolute path generated by mkstemp

    if args.output is None:
        args.output = f"benchmark_statistics_{args.model}.json"

    results = []

    total_iterations = args.runs + args.warmup
    print(
        f"Running {total_iterations} iterations for {args.model.upper()} ({args.warmup} warmup + {args.runs} measured)...\n"
    )

    try:
        for i in range(total_iterations):
            is_warmup = i < args.warmup
            prefix = "[WARMUP]" if is_warmup else f"[{i - args.warmup + 1}/{args.runs}]"

            print(f"{prefix} Running benchmark...")
            result = run_single_benchmark(args.test_script, args.results_file)

            if result:
                if not is_warmup:
                    results.append(result)
                print(
                    f"  ✓ Completed: {result['summary']['total_tokens']} tokens in {result['summary']['total_duration_s']:.1f}s"
                )
                print(
                    f"    Throughput: {result['summary']['avg_tokens_per_second']:.1f} tok/s\n"
                )
            else:
                print("  ✗ Failed\n")

            # Brief pause between runs
            if i < total_iterations - 1:
                time.sleep(2)
    finally:
        # Cleanup temporary file
        if is_temp_file and os.path.exists(args.results_file):
            with contextlib.suppress(OSError):
                os.remove(args.results_file)

    if not results:
        print("[ERROR] No successful runs")
        sys.exit(1)

    # Compute statistics
    print("=" * 70)
    print(f"BENCHMARK STATISTICS - {args.model.upper()}")
    print("=" * 70)
    print(f"\nRuns completed: {len(results)}")

    # Overall throughput stats
    throughputs = [r["summary"]["avg_tokens_per_second"] for r in results]
    print("\nOverall Throughput:")
    print(f"  Mean:   {mean(throughputs):.1f} tok/s")
    if len(throughputs) > 1:
        print(f"  StdDev: {stdev(throughputs):.1f} tok/s")
        print(f"  Min:    {min(throughputs):.1f} tok/s")
        print(f"  Max:    {max(throughputs):.1f} tok/s")

    # Per-test statistics
    print("\nPer-Test Statistics:")
    print(f"{'Test':<25} {'Mean (tok/s)':<15} {'StdDev':<10} {'Min':<10} {'Max':<10}")
    print("-" * 70)

    # Collect per-test data
    test_names = [t["test_name"] for t in results[0]["tests"]]

    for test_name in test_names:
        test_throughputs = []
        for result in results:
            for test in result["tests"]:
                if test["test_name"] == test_name:
                    test_throughputs.append(test["tokens_per_second"])
                    break

        if test_throughputs:
            mean_val = mean(test_throughputs)
            if len(test_throughputs) > 1:
                std_val = stdev(test_throughputs)
                min_val = min(test_throughputs)
                max_val = max(test_throughputs)
                print(
                    f"{test_name:<25} {mean_val:>10.1f}      {std_val:>6.1f}     {min_val:>6.1f}     {max_val:>6.1f}"
                )
            else:
                print(
                    f"{test_name:<25} {mean_val:>10.1f}      {'N/A':>6}     {'N/A':>6}     {'N/A':>6}"
                )

    print("=" * 70)

    # Save aggregated results
    aggregated = {
        "num_runs": len(results),
        "warmup_runs": args.warmup,
        "measured_runs": args.runs,
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
        "model": results[0]["model"],
        "platform": results[0]["platform"],
        "statistics": {
            "overall": {
                "mean_throughput": mean(throughputs),
                "std_throughput": stdev(throughputs) if len(throughputs) > 1 else 0,
                "min_throughput": min(throughputs),
                "max_throughput": max(throughputs),
            },
            "per_test": {},
        },
        "raw_results": results,
    }

    for test_name in test_names:
        test_throughputs = []
        for result in results:
            for test in result["tests"]:
                if test["test_name"] == test_name:
                    test_throughputs.append(test["tokens_per_second"])
                    break

        aggregated["statistics"]["per_test"][test_name] = {
            "mean": mean(test_throughputs),
            "std": stdev(test_throughputs) if len(test_throughputs) > 1 else 0,
            "min": min(test_throughputs),
            "max": max(test_throughputs),
        }

    output_path = Path(__file__).parent / args.output

    # Load existing results if they exist
    existing_data = []
    if output_path.exists():
        try:
            with open(output_path, "r") as f:
                content = json.load(f)
                if isinstance(content, list):
                    existing_data = content
                elif isinstance(content, dict):
                    existing_data = [content]
        except json.JSONDecodeError:
            pass

    # Remove any existing entry for this platform to avoid duplicates
    existing_data = [
        entry
        for entry in existing_data
        if entry.get("platform") != aggregated["platform"]
    ]

    # Append new results
    existing_data.append(aggregated)

    with open(output_path, "w") as f:
        json.dump(existing_data, f, indent=2)

    print(f"\n[INFO] Statistics saved to: {output_path}")


if __name__ == "__main__":
    main()
